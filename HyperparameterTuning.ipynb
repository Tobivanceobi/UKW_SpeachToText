{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# UKW Marine Radio Chatter - Bridge 2 Bridge Communication\n",
    "This notebook uses pretrained models to transcribe the audio files from the UKW Marine Radio Chatter - Bridge 2 Bridge Communication dataset. <br>\n",
    "The dataset contains audio files and their corresponding transcriptions. Further we classify the speakers contained in the audio files."
   ],
   "id": "118ce1db4c0bea85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T16:04:49.724782Z",
     "start_time": "2024-05-26T16:04:45.645761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import IPython\n",
    "import torchaudio\n",
    "import torch\n",
    "import wandb\n",
    "from pydub import AudioSegment\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from src.utils import txt_to_dataframe\n",
    "import numpy as np"
   ],
   "id": "152a0b8bde3010d9",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configuration - Data Directories",
   "id": "fc6d817c34a305be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T16:04:49.729188Z",
     "start_time": "2024-05-26T16:04:49.726054Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Config:\n",
    "    DATA_DIR = '../data/'\n",
    "    AUDIO_DIR = DATA_DIR + 'audio/'\n",
    "    TEXT_DIR = DATA_DIR + 'text/'\n",
    "    DATASET_DIR = 'dataset/'\n",
    "    \n",
    "    KAGGLE_DATA_TAG = 'linogova/marine-radio-chatter-bridge-2-bridge-communication/1'\n",
    "    KAGGLE_DATA_DIR = 'Marine_audio/'\n",
    "\n",
    "config = Config()"
   ],
   "id": "b08f2d6491009490",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T16:04:49.736338Z",
     "start_time": "2024-05-26T16:04:49.730249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torchaudio\n",
    "import torch\n",
    "\n",
    "def process_audio_segments(meta_df, waveform, sample_rate, padding=0.1):\n",
    "    # Prepare lists to hold the processed data\n",
    "    new_rows = []\n",
    "    audio_segments = []\n",
    "    \n",
    "    # Initialize variables to track the current segment\n",
    "    current_start = None\n",
    "    current_end = None\n",
    "    current_transcript = \"\"\n",
    "    current_audio = []\n",
    "    current_length = 0  # Length in seconds\n",
    "    \n",
    "    for index, row in meta_df.iterrows():\n",
    "        transcript = row['transcript']\n",
    "        start_time = row['start_time'] - padding\n",
    "        end_time = row['end_time'] + padding\n",
    "        \n",
    "        # If the padding overlaps with the previous segment, adjust the start time\n",
    "        if current_end is not None and row['start_time']- current_end < padding:\n",
    "             start_time = current_end\n",
    "            \n",
    "        # If the padding overlaps with the next segment, adjust the end time\n",
    "        if index < len(meta_df) - 1 and meta_df.iloc[index + 1]['start_time'] - end_time < padding:\n",
    "            end_time = ((meta_df.iloc[index + 1]['start_time'] - end_time) / 2) + end_time\n",
    "            \n",
    "        # Calculate the duration of the current row's audio segment\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        # If adding this segment exceeds 30 seconds, save the current segment and start a new one\n",
    "        if current_length + duration > 30:\n",
    "            if current_start is not None:\n",
    "                new_rows.append({\n",
    "                    'start_time': current_start,\n",
    "                    'end_time': current_end,\n",
    "                    'transcript': current_transcript\n",
    "                })\n",
    "                audio_segments.append(torch.cat(current_audio, dim=1).squeeze())\n",
    "            \n",
    "            # Reset for the new segment\n",
    "            current_start = start_time\n",
    "            current_end = end_time\n",
    "            current_transcript = transcript\n",
    "            current_audio = [waveform[:, int(start_time * sample_rate):int(end_time * sample_rate)]]\n",
    "            current_length = duration\n",
    "        else:\n",
    "            # If it doesn't exceed 30 seconds, update the current segment\n",
    "            if current_start is None:\n",
    "                current_start = start_time\n",
    "            current_end = end_time\n",
    "            current_transcript += \" \" + str(transcript)\n",
    "            current_audio.append(waveform[:, int(start_time * sample_rate):int(end_time * sample_rate)])\n",
    "            current_length += duration\n",
    "    \n",
    "    # Add the last segment if any\n",
    "    if current_start is not None:\n",
    "        new_rows.append({\n",
    "            'start_time': current_start,\n",
    "            'end_time': current_end,\n",
    "            'transcript': current_transcript\n",
    "        })\n",
    "        audio_segments.append(torch.cat(current_audio, dim=1).squeeze())\n",
    "    \n",
    "    return new_rows, audio_segments\n",
    "\n",
    "# data_ids = [f.replace(\".wav\", \"\") for f in os.listdir(config.DATASET_DIR + \"audio\")]\n",
    "\n",
    "# for idx in range(len(data_ids)):\n",
    "#     if idx % 10 == 0:\n",
    "#         print(f\"Processing {idx}/{len(data_ids)}\")\n",
    "#     audio_fpath = os.path.join(config.DATASET_DIR, f\"audio/{data_ids[idx]}.wav\")\n",
    "#     text_fpath = os.path.join(config.DATASET_DIR, f\"text/{data_ids[idx]}.csv\")\n",
    "#     \n",
    "#     waveform, sample_rate = torchaudio.load(audio_fpath)\n",
    "#     waveform = waveform.float()\n",
    "#     transcripts_df = pd.read_csv(text_fpath)\n",
    "#     \n",
    "#     target_segments, audio_segments = process_audio_segments(transcripts_df, waveform, sample_rate)"
   ],
   "id": "a3a3c31e70d0fd4a",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T16:04:50.270730Z",
     "start_time": "2024-05-26T16:04:49.738285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy import signal\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "from joblib import Parallel, delayed\n",
    "from torch.utils.data import Dataset\n",
    "from src.utils import bcolors\n",
    "\n",
    "c = bcolors()\n",
    "\n",
    "\n",
    "def lowpass_filter(audio_data, sr):\n",
    "    # Create a lowpass filter\n",
    "    b, a = signal.butter(4, 1300, 'low', fs=sr)\n",
    "    # Apply the lowpass filter\n",
    "    filtered_audio_data = signal.filtfilt(b, a, audio_data)\n",
    "    return filtered_audio_data\n",
    "\n",
    "def apply_rms_normalization(waveform):\n",
    "    rms_value = waveform.pow(2).mean().sqrt()  # Calculate RMS value of the waveform\n",
    "    target_rms = 0.1  # Example target RMS value\n",
    "    normalized_waveform = waveform * (target_rms / rms_value)  # Scale waveform to desired RMS value\n",
    "    return normalized_waveform\n",
    "\n",
    "class UKWFunkSprache(Dataset):\n",
    "    def __init__(self, \n",
    "                 file_ids, \n",
    "                 root_dir, \n",
    "                 proc=None,\n",
    "                 padding=None,\n",
    "                 rms_norm=False,\n",
    "                 filter_data=False,\n",
    "                 n_jobs=-1):\n",
    "        self.feed_ids = file_ids\n",
    "        self.root_dir = root_dir\n",
    "        self.processor = proc\n",
    "        self.rms_norm = rms_norm\n",
    "        self.filter_data = filter_data\n",
    "        self.padding = padding\n",
    "\n",
    "        print(f\"\\n{c.OKGREEN}Preloading Samples...{c.ENDC}\")\n",
    "        print(f\"\\n{c.OKCYAN}Audio Files:         {len(self.feed_ids)}{c.ENDC}\")\n",
    "        print(f\"{c.OKCYAN}Jobs:                {n_jobs} {c.ENDC}\\n\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        result = []\n",
    "        for idx in range(len(self.feed_ids)):\n",
    "            result.append(self.process_file(idx))\n",
    "        # result = Parallel(n_jobs=n_jobs)(\n",
    "        #     delayed(self.process_file)(idx) for idx in range(len(self.feed_ids))\n",
    "        # )\n",
    "        result = [item for sublist in result for item in sublist]\n",
    "        print(f\"\\n{c.OKGREEN}Preloading Complete!{c.ENDC}\")\n",
    "\n",
    "        self.audio_samples = [item['audio'] for item in result]\n",
    "        self.transcriptions = [item['transcript'] for item in result]\n",
    "        self.groups = [item['group'] for item in result]\n",
    "        \n",
    "        print(f\"{c.OKCYAN}Number of Samples:   {len(self.audio_samples)} {c.ENDC}\\n\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        t = end_time - start_time\n",
    "        print(f\"\\n{c.OKBLUE}Time taken:      {int((t - (t % 60)) / 60)} min {t % 60} sec {c.ENDC}\")\n",
    "\n",
    "    def process_file(self, idx):\n",
    "        feed_id = self.feed_ids[idx]\n",
    "        audio_fpath = os.path.join(self.root_dir, f\"audio/{feed_id}.wav\")\n",
    "        text_fpath = os.path.join(self.root_dir, f\"text/{feed_id}.csv\")\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(audio_fpath, channels_first=True)\n",
    "        waveform = waveform.float()\n",
    "        transcripts_df = pd.read_csv(text_fpath)\n",
    "        \n",
    "        if self.rms_norm:\n",
    "            waveform = apply_rms_normalization(waveform)\n",
    "        \n",
    "        if self.filter_data:\n",
    "            waveform = lowpass_filter(waveform, sample_rate)\n",
    "            \n",
    "        audio_dur = transcripts_df.iloc[-1]['end_time'] - transcripts_df.iloc[0]['start_time']\n",
    "        if audio_dur < 10:\n",
    "            return []\n",
    "        \n",
    "        # batches = batch_data(transcripts_df.to_dict(\"records\"), waveform)\n",
    "        target_segments, audio_segments = process_audio_segments(transcripts_df, waveform, sample_rate, padding=self.padding)\n",
    "        if len(audio_segments) == 0:\n",
    "            return []\n",
    "        \n",
    "        samples = []\n",
    "        \n",
    "        for i in range(len(target_segments)):\n",
    "            if self.processor:\n",
    "                target = self.processor.tokenizer(target_segments[i]['transcript'], return_tensors=\"pt\").input_ids.squeeze(0)\n",
    "                audio = self.processor.feature_extractor(audio_segments[i], sampling_rate=sample_rate, return_tensors=\"pt\").input_features.squeeze(0)\n",
    "                samples.append({\n",
    "                    'group': str(feed_id),\n",
    "                    'audio': audio,\n",
    "                    'transcript': target\n",
    "                })\n",
    "            else:\n",
    "                samples.append({\n",
    "                    'group': str(feed_id),\n",
    "                    'audio': audio_segments[i],\n",
    "                    'transcript': target_segments[i]['transcript']\n",
    "                })\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        audio = self.audio_samples[idx]\n",
    "        transcript = self.transcriptions[idx]\n",
    "\n",
    "        return {\n",
    "            \"input_features\": audio,\n",
    "            \"labels\": transcript\n",
    "        }\n"
   ],
   "id": "d0d540d77cdf2be2",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "2c2f8a248e0826c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T16:04:50.457035Z",
     "start_time": "2024-05-26T16:04:50.271729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchmetrics\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import pytorch_lightning as pl\n",
    "import evaluate\n",
    "\n",
    "\n",
    "class WhisperLightningModule(pl.LightningModule):\n",
    "    def __init__(self, model_name: str, processor, learning_rate: float, weight_decay: float, warmup_steps: int, num_jobs: int = 8):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.processor = processor\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.num_jobs = num_jobs\n",
    "\n",
    "        self.model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "        self.wer = torchmetrics.text.wer.WordErrorRate()\n",
    "        self.best_val_loss = float(\"inf\")\n",
    "        self.val_loss = []\n",
    "        self.val_preds = []\n",
    "        self.val_true = []\n",
    "\n",
    "    def forward(self, input_features, labels):\n",
    "        return self.model(input_features=input_features, labels=labels)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(batch[\"input_features\"], batch[\"labels\"])\n",
    "        loss = outputs.loss\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self(batch[\"input_features\"], batch[\"labels\"])\n",
    "        loss = outputs.loss\n",
    "        self.val_loss.append(loss)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "\n",
    "    #     self.val_preds.append(outputs.logits.argmax(-1))\n",
    "    #     self.val_true.append(batch[\"labels\"])\n",
    "\n",
    "    # def on_validation_epoch_end(self) -> None:\n",
    "    #     preds_decoded = [self.processor.decode(pred, skip_special_tokens=True) for b in self.val_preds for pred in b]\n",
    "    #     true_decoded = [self.processor.decode(true, skip_special_tokens=True) for b in self.val_true for true in b]\n",
    "    #     # self.val_preds = Parallel(n_jobs=self.num_jobs)(\n",
    "    #     #     delayed(self.processor.decode)(pred, skip_special_tokens=True) for b in self.val_preds for pred in b\n",
    "    #     # )\n",
    "    #     # self.val_true = Parallel(n_jobs=self.num_jobs)(\n",
    "    #     #     delayed(self.processor.decode)(true, skip_special_tokens=True) for b in self.val_true for true in b\n",
    "    #     # )\n",
    "    #\n",
    "    #     wer = self.wer(preds_decoded, true_decoded)\n",
    "    #     self.log(\"val_wer\", wer, prog_bar=True)\n",
    "    #\n",
    "    #     self.val_preds = []\n",
    "    #     self.val_true = []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.warmup_steps,\n",
    "            num_training_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]"
   ],
   "id": "1270ef37127a0362",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T16:04:50.463366Z",
     "start_time": "2024-05-26T16:04:50.458088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class SpeechDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_dataset, val_dataset, processor, batch_size: int, num_workers: int = 8):\n",
    "        super().__init__()\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.processor = processor\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, collate_fn=self.collate_fn, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, collate_fn=self.collate_fn, num_workers=self.num_workers)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, collate_fn=self.collate_fn, num_workers=self.num_workers)\n",
    "\n",
    "    def collate_fn(self, features):\n",
    "        input_features = [feature[\"input_features\"] for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            [{\"input_features\": input_feature} for input_feature in input_features],\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels = [feature[\"labels\"] for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            [{\"input_ids\": label} for label in labels],\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        if (labels[:, 0] == self.processor.tokenizer.pad_token_id).all():\n",
    "            labels = labels[:, 1:]\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n"
   ],
   "id": "334f9c383e50aef2",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hyperparameter Finetuning",
   "id": "b5f054d8f6f189c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T16:04:50.724214Z",
     "start_time": "2024-05-26T16:04:50.464617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "model_config = {\n",
    "    \"model_name\": \"openai/whisper-tiny\",\n",
    "}\n",
    "\n",
    "# Initialize the processor\n",
    "processor = WhisperProcessor.from_pretrained(\n",
    "    model_config[\"model_name\"], \n",
    "    language='en', \n",
    "    task=\"transcribe\", \n",
    "    do_normalize=True, \n",
    "    sampling_rate=16000, \n",
    "    return_tensors=\"pt\", \n",
    "    device=\"cpu\",\n",
    "    local_files_only=True\n",
    ")"
   ],
   "id": "36f10e79ecde3ba",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/Desktop/Uni/SS24/NLP/UKW_SpeachToText/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T16:05:44.985128Z",
     "start_time": "2024-05-26T16:04:50.725211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_config[\"filter_data\"] = False\n",
    "model_config[\"rms_norm\"] = False\n",
    "\n",
    "# Create the Datasets\n",
    "feed_ids = [f.replace(\".wav\", \"\") for f in os.listdir(config.DATASET_DIR + \"audio\")]\n",
    "\n",
    "ds_train = UKWFunkSprache(\n",
    "    feed_ids[:1700], \n",
    "    config.DATASET_DIR, \n",
    "    proc=processor, \n",
    "    filter_data=model_config[\"filter_data\"], \n",
    "    rms_norm=model_config[\"rms_norm\"],\n",
    "    padding=0.5\n",
    ")\n",
    "ds_val = UKWFunkSprache(\n",
    "    feed_ids[1700:], \n",
    "    config.DATASET_DIR, \n",
    "    proc=processor, \n",
    "    filter_data=model_config[\"filter_data\"], \n",
    "    rms_norm=model_config[\"rms_norm\"],\n",
    "    padding=0.5\n",
    ")\n",
    "\n",
    "model_config[\"num_train_samples\"] = len(ds_train)\n",
    "model_config[\"num_val_samples\"] = len(ds_val)"
   ],
   "id": "66e42bf085eb5ee5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[92mPreloading Samples...\u001B[0m\n",
      "\n",
      "\u001B[96mAudio Files:         1700\u001B[0m\n",
      "\u001B[96mJobs:                -1 \u001B[0m\n",
      "\n",
      "\n",
      "\u001B[92mPreloading Complete!\u001B[0m\n",
      "\u001B[96mNumber of Samples:   2938 \u001B[0m\n",
      "\n",
      "\n",
      "\u001B[94mTime taken:      0 min 46.50473380088806 sec \u001B[0m\n",
      "\n",
      "\u001B[92mPreloading Samples...\u001B[0m\n",
      "\n",
      "\u001B[96mAudio Files:         300\u001B[0m\n",
      "\u001B[96mJobs:                -1 \u001B[0m\n",
      "\n",
      "\n",
      "\u001B[92mPreloading Complete!\u001B[0m\n",
      "\u001B[96mNumber of Samples:   511 \u001B[0m\n",
      "\n",
      "\n",
      "\u001B[94mTime taken:      0 min 7.73941707611084 sec \u001B[0m\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T16:05:45.059742Z",
     "start_time": "2024-05-26T16:05:44.986501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "import optuna\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "\n",
    "\n",
    "def train_model():\n",
    "    wandb.init()\n",
    "    \n",
    "    # Initialize the WandbLogger\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"ukw-radio-trans_\" + model_config[\"model_name\"].split(\"/\")[-1], \n",
    "        name=f\"lr_{wandb.config.lr:.6f}_wd_{wandb.config.weight_decay:.6f}\",\n",
    "        log_model=False\n",
    "    )\n",
    "    wandb.require(experiment=\"service\") \n",
    "    \n",
    "    data_module = SpeechDataModule(ds_train, ds_val, processor, wandb.config.batch_size)\n",
    "    \n",
    "    # Initialize the model with suggested hyperparameters\n",
    "    model_train = WhisperLightningModule(model_config[\"model_name\"], processor, wandb.config.lr, wandb.config.weight_decay, wandb.config.warmup_steps)\n",
    "    model_train.model = WhisperForConditionalGeneration.from_pretrained(model_config[\"model_name\"])\n",
    "    model_train.model.generation_config.language = \"en\"\n",
    "    model_train.model.generation_config.task = \"transcribe\"\n",
    "    model_train.model.generation_config.is_multilingual = False\n",
    "    model_train.model.generation_config.temperature = (0, 0.2, 0.4, 0.6, 0.8, 1.0)\n",
    "    model_train.model.generation_config.compression_ratio_threshold = wandb.config.cr_threshold\n",
    "    \n",
    "    # Freeze or unfreeze layers based on the original configuration\n",
    "    for param in model_train.model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    # Freeze layers in the decoder\n",
    "    for param in model_train.model.model.decoder.parameters():\n",
    "        param.requires_grad = wandb.config.unfreeze_decoder\n",
    "        \n",
    "    # Freeze layers in the encoder\n",
    "    for param in model_train.model.model.encoder.parameters():\n",
    "        param.requires_grad = wandb.config.unfreeze_encoder\n",
    "        \n",
    "    # Freeze layers in the linear layer\n",
    "    model_train.model.proj_out.weight.requires_grad = wandb.config.unfreeze_linear\n",
    "    \n",
    "    # Initialize Early Stopping monitor the difference between the training and validation loss\n",
    "    early_stopping = pl.callbacks.EarlyStopping(\"val_loss_epoch\", patience=1, mode=\"min\", min_delta=0.05, verbose=False)\n",
    "    \n",
    "    # Initialize the Trainer with WandbLogger\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=wandb.config.n_epochs,\n",
    "        logger=wandb_logger,\n",
    "        accelerator=\"auto\",\n",
    "        log_every_n_steps=5,\n",
    "        num_sanity_val_steps=5,\n",
    "        callbacks=[early_stopping],\n",
    "        enable_model_summary=False,\n",
    "        enable_checkpointing=False\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.fit(model_train, data_module)\n",
    "    val_loss = trainer.callback_metrics[\"val_loss_epoch\"].item()\n",
    "    \n",
    "    # Free up memory\n",
    "    del model_train\n",
    "    del data_module\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Return the validation loss\n",
    "    return val_loss\n"
   ],
   "id": "8fd455d73f82a87f",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T16:38:04.450670Z",
     "start_time": "2024-05-26T16:06:24.572919Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sweep_config = {\n",
    "    'method': 'bayes',\n",
    "    'name': 'version-2',\n",
    "    'metric': {\n",
    "        'goal': 'minimize',\n",
    "        'name': 'val_loss_epoch'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'lr': {'max': 0.0001, 'min': 0.00005},\n",
    "        'weight_decay': {'max': 0.005, 'min': 0.0001},\n",
    "        'batch_size': {'values': [8]},\n",
    "        'warmup_steps': {'values': [400, 800]},\n",
    "        'n_epochs': {'values': [3]},\n",
    "        'unfreeze_encoder': {'values': [False]},\n",
    "        'unfreeze_decoder': {'values': [True]},\n",
    "        'unfreeze_linear': {'values': [True]},\n",
    "        'cr_threshold': {'values': [1.2, 1.35, 1.5]}\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id=wandb.sweep(sweep_config, project=\"hp_tuning_\" + model_config[\"model_name\"].split(\"/\")[-1])\n",
    "wandb.agent(sweep_id=sweep_id, function=train_model, count=20)"
   ],
   "id": "b42863e82d31861a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: ams4q60w\n",
      "Sweep URL: https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Agent Starting Run: l5wczxkt with config:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbatch_size: 8\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tcr_threshold: 1.5\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tlr: 8.429574733397666e-05\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tn_epochs: 3\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tunfreeze_decoder: True\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tunfreeze_encoder: False\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tunfreeze_linear: True\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \twarmup_steps: 800\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tweight_decay: 0.003907846891057378\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mtobias-ettling\u001B[0m (\u001B[33mtobias-ettling-wandb\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/tobias/Desktop/Uni/SS24/NLP/UKW_SpeachToText/wandb/run-20240526_180627-l5wczxkt</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/l5wczxkt' target=\"_blank\">volcanic-sweep-1</a></strong> to <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View sweep at <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/l5wczxkt' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/l5wczxkt</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/Desktop/Uni/SS24/NLP/UKW_SpeachToText/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/tobias/Desktop/Uni/SS24/NLP/UKW_SpeachToText/.venv/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Config item 'warmup_steps' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8d8abd851bb241f48388d4bb62aacab4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2da85ca2837747f38186dd91961a45d3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "afcae5f664cd470086c9565f450e737d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a22bd8cf67474781b9d2d6a81fd5181b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "83a74fbc8b3a4961a587239f23bbd025"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▁</td></tr><tr><td>train_loss_step</td><td>█▆▆▅▇▄▃▃▃▄▃▃▂▃▃▂▃▄▂▁▁▁▁▂▁▄▂▂▂▁▂▂▂▂▂▂▁▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▃▃▃▄▄▄▁▁▁▁▁▁▁▁▂▅▅▅▆▆▆▇▇▇██▂▂▂▂▂▂▂▂▂</td></tr><tr><td>val_loss_epoch</td><td>█▁</td></tr><tr><td>val_loss_step</td><td>▂▃▄▁▄▃▄█▇▆▃▅▂▃▂▃▅▂▃▂▁▃▄▆▃▄▂█▁▆▄▁▁▄▂▃▃▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>train_loss_epoch</td><td>1.00144</td></tr><tr><td>train_loss_step</td><td>0.68545</td></tr><tr><td>trainer/global_step</td><td>735</td></tr><tr><td>val_loss_epoch</td><td>1.13981</td></tr><tr><td>val_loss_step</td><td>1.09186</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">volcanic-sweep-1</strong> at: <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/l5wczxkt' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/l5wczxkt</a><br/> View project at: <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20240526_180627-l5wczxkt/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Sweep Agent: Waiting for job.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Job received.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Agent Starting Run: osuo2itu with config:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbatch_size: 8\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tcr_threshold: 1.2\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tlr: 6.331437008602591e-05\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tn_epochs: 3\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tunfreeze_decoder: True\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tunfreeze_encoder: False\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tunfreeze_linear: True\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \twarmup_steps: 800\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tweight_decay: 0.0034848868475450745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/tobias/Desktop/Uni/SS24/NLP/UKW_SpeachToText/wandb/run-20240526_180913-osuo2itu</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/osuo2itu' target=\"_blank\">bright-sweep-2</a></strong> to <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View sweep at <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/osuo2itu' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/osuo2itu</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/Desktop/Uni/SS24/NLP/UKW_SpeachToText/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/tobias/Desktop/Uni/SS24/NLP/UKW_SpeachToText/.venv/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Config item 'warmup_steps' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2deb7ce62d274525beb0d299055a9e9f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db7ec9b438214103aa3c8bdf30e434fa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "55403b5071b24cf799664c28461e6fec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "20e1aec27bdd48c4844866f90c775f71"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2ae539288053408ca2f0509442a9cfea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a03fe02367dd488ca25a48b37aa4d8cd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅█████████████</td></tr><tr><td>train_loss_epoch</td><td>█▃▁</td></tr><tr><td>train_loss_step</td><td>█▆▅▅▄▄▅▃▃▄▃▃▃▂▃▃▂▂▃▂▃▅▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▃▃▁▁▁▁▁▁▃▄▄▄▅▅▅▆▁▂▂▂▂▂▆▆▇▇▇██▂▂▂▂▂▂</td></tr><tr><td>val_loss_epoch</td><td>█▅▁</td></tr><tr><td>val_loss_step</td><td>▃▃▂▃▄█▇▆▂▄▃▄▄▅▃▂▄▄█▆▄▂▄▃▄▅▄▃▂▃▄█▁▃▁▄▄▃▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>train_loss_epoch</td><td>0.76394</td></tr><tr><td>train_loss_step</td><td>0.50226</td></tr><tr><td>trainer/global_step</td><td>1103</td></tr><tr><td>val_loss_epoch</td><td>1.06426</td></tr><tr><td>val_loss_step</td><td>1.19939</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">bright-sweep-2</strong> at: <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/osuo2itu' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/osuo2itu</a><br/> View project at: <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20240526_180913-osuo2itu/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Agent Starting Run: 8zu8k5z8 with config:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbatch_size: 8\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tcr_threshold: 1.5\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tlr: 8.127392276847558e-05\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tn_epochs: 3\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tunfreeze_decoder: True\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tunfreeze_encoder: False\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tunfreeze_linear: True\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \twarmup_steps: 800\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tweight_decay: 0.0013632377464585649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/tobias/Desktop/Uni/SS24/NLP/UKW_SpeachToText/wandb/run-20240526_181301-8zu8k5z8</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/8zu8k5z8' target=\"_blank\">noble-sweep-3</a></strong> to <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View sweep at <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/8zu8k5z8' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/8zu8k5z8</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/Desktop/Uni/SS24/NLP/UKW_SpeachToText/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/tobias/Desktop/Uni/SS24/NLP/UKW_SpeachToText/.venv/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Config item 'warmup_steps' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "131b996442f044938318f5185854d535"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff67fbe4490d4b8d832763a8861ef0bf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce29628ea5ed41028714c49033f0583c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3cbfefb26ae94499a362b64287ef0a30"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a0774fd1bd3444b196e28e421869ea5e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▁</td></tr><tr><td>train_loss_step</td><td>█▆▆▅▇▄▃▃▃▄▃▃▂▃▃▂▃▄▂▁▁▁▁▂▁▄▂▂▂▁▂▂▂▂▂▂▁▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▃▃▃▄▄▄▁▁▁▁▁▁▁▁▂▅▅▅▆▆▆▇▇▇██▂▂▂▂▂▂▂▂▂</td></tr><tr><td>val_loss_epoch</td><td>█▁</td></tr><tr><td>val_loss_step</td><td>▂▃▄▁▄▃▄█▇▆▃▅▂▃▂▃▅▂▃▂▁▃▄▆▃▄▂█▁▆▄▁▁▄▂▃▃▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>train_loss_epoch</td><td>1.0044</td></tr><tr><td>train_loss_step</td><td>0.69763</td></tr><tr><td>trainer/global_step</td><td>735</td></tr><tr><td>val_loss_epoch</td><td>1.14376</td></tr><tr><td>val_loss_step</td><td>1.10966</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">noble-sweep-3</strong> at: <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/8zu8k5z8' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/8zu8k5z8</a><br/> View project at: <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20240526_181301-8zu8k5z8/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Agent Starting Run: u1i9xjv5 with config:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbatch_size: 8\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tcr_threshold: 1.2\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tlr: 5.905986501916375e-05\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tn_epochs: 3\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tunfreeze_decoder: True\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tunfreeze_encoder: False\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tunfreeze_linear: True\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \twarmup_steps: 800\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tweight_decay: 0.0031373657992394195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/tobias/Desktop/Uni/SS24/NLP/UKW_SpeachToText/wandb/run-20240526_181542-u1i9xjv5</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/u1i9xjv5' target=\"_blank\">ruby-sweep-4</a></strong> to <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View sweep at <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/u1i9xjv5' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/u1i9xjv5</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/Desktop/Uni/SS24/NLP/UKW_SpeachToText/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/tobias/Desktop/Uni/SS24/NLP/UKW_SpeachToText/.venv/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Config item 'warmup_steps' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "505a1cfc90b84612a360d96ecbc09d48"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "86692ea299964fd890a3c1d593701b8e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9fe5e96831cf44d78054720f3cc412f6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0923725eaed042c49c1548ad036f62e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dcf7e324fee64527b93bed3e9f21a200"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2a60abc2c517472da0eaf265b2e79f7b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅█████████████</td></tr><tr><td>train_loss_epoch</td><td>█▃▁</td></tr><tr><td>train_loss_step</td><td>█▆▅▅▄▄▅▃▃▄▃▃▃▂▃▃▂▂▃▂▃▅▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▃▃▁▁▁▁▁▁▃▄▄▄▅▅▅▆▁▂▂▂▂▂▆▆▇▇▇██▂▂▂▂▂▂</td></tr><tr><td>val_loss_epoch</td><td>█▄▁</td></tr><tr><td>val_loss_step</td><td>▃▃▂▃▄█▇▆▂▄▄▄▄▅▃▂▄▄█▆▄▂▄▃▄▅▄▃▂▃▄█▁▃▁▄▄▃▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>train_loss_epoch</td><td>0.77517</td></tr><tr><td>train_loss_step</td><td>0.51539</td></tr><tr><td>trainer/global_step</td><td>1103</td></tr><tr><td>val_loss_epoch</td><td>1.06287</td></tr><tr><td>val_loss_step</td><td>1.2412</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">ruby-sweep-4</strong> at: <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/u1i9xjv5' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/u1i9xjv5</a><br/> View project at: <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20240526_181542-u1i9xjv5/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Sweep Agent: Waiting for job.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Job received.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Agent Starting Run: mi60mzom with config:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbatch_size: 8\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tcr_threshold: 1.2\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tlr: 5.245358269258695e-05\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tn_epochs: 3\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tunfreeze_decoder: True\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tunfreeze_encoder: False\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tunfreeze_linear: True\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \twarmup_steps: 800\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tweight_decay: 0.0035430987113642017\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/tobias/Desktop/Uni/SS24/NLP/UKW_SpeachToText/wandb/run-20240526_181946-mi60mzom</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/mi60mzom' target=\"_blank\">polar-sweep-5</a></strong> to <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View sweep at <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/mi60mzom' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/mi60mzom</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/Desktop/Uni/SS24/NLP/UKW_SpeachToText/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/tobias/Desktop/Uni/SS24/NLP/UKW_SpeachToText/.venv/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Config item 'warmup_steps' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2fe44b70d4cb482babcb2dbfec6367e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "280017beaf3e431699eab56d430aa233"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f16882654bb4b6790270a45ff6d2078"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "840ff1906f1f4d0ca757f013d7cdd8cb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "923dc7f076354428931251cd7b56a3ec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ee06db969a89479088a67274bfccea3f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅█████████████</td></tr><tr><td>train_loss_epoch</td><td>█▃▁</td></tr><tr><td>train_loss_step</td><td>█▆▅▅▄▄▅▃▃▄▃▃▃▂▃▃▂▂▃▂▃▅▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▃▃▁▁▁▁▁▁▃▄▄▄▅▅▅▆▁▂▂▂▂▂▆▆▇▇▇██▂▂▂▂▂▂</td></tr><tr><td>val_loss_epoch</td><td>█▄▁</td></tr><tr><td>val_loss_step</td><td>▃▄▂▃▅█▇▆▂▄▄▄▄▅▃▂▄▄█▆▄▂▄▃▄▅▄▃▁▃▄█▁▃▁▄▄▃▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>train_loss_epoch</td><td>0.79421</td></tr><tr><td>train_loss_step</td><td>0.53765</td></tr><tr><td>trainer/global_step</td><td>1103</td></tr><tr><td>val_loss_epoch</td><td>1.06219</td></tr><tr><td>val_loss_step</td><td>1.26057</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">polar-sweep-5</strong> at: <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/mi60mzom' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/mi60mzom</a><br/> View project at: <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20240526_181946-mi60mzom/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Agent Starting Run: k6p4ggq0 with config:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbatch_size: 8\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tcr_threshold: 1.2\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tlr: 5.9289937106594744e-05\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tn_epochs: 3\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tunfreeze_decoder: True\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tunfreeze_encoder: False\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tunfreeze_linear: True\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \twarmup_steps: 800\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tweight_decay: 0.0025586121805272655\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/tobias/Desktop/Uni/SS24/NLP/UKW_SpeachToText/wandb/run-20240526_182341-k6p4ggq0</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/k6p4ggq0' target=\"_blank\">fiery-sweep-6</a></strong> to <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View sweep at <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/k6p4ggq0' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/k6p4ggq0</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/Desktop/Uni/SS24/NLP/UKW_SpeachToText/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/tobias/Desktop/Uni/SS24/NLP/UKW_SpeachToText/.venv/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Config item 'warmup_steps' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd0f1c31cf9340238dc0bb8c0ab54c05"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "65e7be4c28524dcbb8d5ece5e0b171c1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e810f0988eb4e158b079e25a7286aea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "32472719e97f448cb467d8c2d97d72ba"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d2894d3447594eb29b1ba57ca359f1a7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.008 MB of 0.008 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f6a3b92e239e40f185edbe554fb6cd68"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅█████████████</td></tr><tr><td>train_loss_epoch</td><td>█▃▁</td></tr><tr><td>train_loss_step</td><td>█▆▅▅▄▄▅▃▃▄▃▃▃▂▃▃▂▂▃▂▃▅▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▃▃▁▁▁▁▁▁▃▄▄▄▅▅▅▆▁▂▂▂▂▂▆▆▇▇▇██▂▂▂▂▂▂</td></tr><tr><td>val_loss_epoch</td><td>█▄▁</td></tr><tr><td>val_loss_step</td><td>▃▃▂▃▄█▇▆▂▄▄▄▄▅▃▂▄▄█▆▄▂▄▃▄▅▄▃▂▃▄█▁▃▁▄▄▃▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>train_loss_epoch</td><td>0.77454</td></tr><tr><td>train_loss_step</td><td>0.51462</td></tr><tr><td>trainer/global_step</td><td>1103</td></tr><tr><td>val_loss_epoch</td><td>1.06302</td></tr><tr><td>val_loss_step</td><td>1.24097</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fiery-sweep-6</strong> at: <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/k6p4ggq0' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/k6p4ggq0</a><br/> View project at: <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20240526_182341-k6p4ggq0/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Agent Starting Run: r3vv6mx7 with config:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbatch_size: 8\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tcr_threshold: 1.2\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tlr: 6.798285311945353e-05\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tn_epochs: 3\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tunfreeze_decoder: True\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tunfreeze_encoder: False\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tunfreeze_linear: True\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \twarmup_steps: 800\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tweight_decay: 0.0029279606708938325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/tobias/Desktop/Uni/SS24/NLP/UKW_SpeachToText/wandb/run-20240526_182735-r3vv6mx7</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/r3vv6mx7' target=\"_blank\">zany-sweep-7</a></strong> to <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View sweep at <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/r3vv6mx7' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/r3vv6mx7</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/Desktop/Uni/SS24/NLP/UKW_SpeachToText/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/tobias/Desktop/Uni/SS24/NLP/UKW_SpeachToText/.venv/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Config item 'warmup_steps' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5828abe3f8404b40a293e31c363dfabf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0f72bde089ce470cae4cdb249a66e3ee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f88515fc7024ce080007f4848b4e4c6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1be58f4b80254228bcad166e0035b058"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.008 MB uploaded\\r'), FloatProgress(value=0.3069948186528497, max=1.0…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88602acdc4a44faaaf16b7736705b36c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁████████████████████</td></tr><tr><td>train_loss_epoch</td><td>█▁</td></tr><tr><td>train_loss_step</td><td>█▆▆▅▇▅▃▃▃▄▃▃▂▃▄▂▄▄▂▁▁▁▁▂▁▄▂▂▂▁▂▂▂▂▂▁▂▂▂▂</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▃▃▃▄▄▄▁▁▁▁▁▁▁▁▂▅▅▅▆▆▆▇▇▇██▂▂▂▂▂▂▂▂▂</td></tr><tr><td>val_loss_epoch</td><td>█▁</td></tr><tr><td>val_loss_step</td><td>▂▃▄▁▄▃▄█▇▆▃▅▂▃▂▃▅▂▃▂▁▃▄▆▃▄▂█▁▆▃▁▁▄▂▃▃▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>1</td></tr><tr><td>train_loss_epoch</td><td>1.01842</td></tr><tr><td>train_loss_step</td><td>0.69373</td></tr><tr><td>trainer/global_step</td><td>735</td></tr><tr><td>val_loss_epoch</td><td>1.13026</td></tr><tr><td>val_loss_step</td><td>1.10101</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zany-sweep-7</strong> at: <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/r3vv6mx7' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/r3vv6mx7</a><br/> View project at: <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20240526_182735-r3vv6mx7/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Agent Starting Run: 08fyp9yp with config:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbatch_size: 8\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tcr_threshold: 1.35\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tlr: 6.343620123508794e-05\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tn_epochs: 3\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tunfreeze_decoder: True\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tunfreeze_encoder: False\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tunfreeze_linear: True\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \twarmup_steps: 400\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tweight_decay: 0.00414808776241688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/tobias/Desktop/Uni/SS24/NLP/UKW_SpeachToText/wandb/run-20240526_183011-08fyp9yp</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/08fyp9yp' target=\"_blank\">chocolate-sweep-8</a></strong> to <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View sweep at <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/08fyp9yp' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/08fyp9yp</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/Desktop/Uni/SS24/NLP/UKW_SpeachToText/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/tobias/Desktop/Uni/SS24/NLP/UKW_SpeachToText/.venv/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Config item 'warmup_steps' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab10dcad32a24d5f8527b8f8577911c1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "244ca03c6f1d4af5abf25b4886b0767f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c3003038c51b426c98f4cc41dba8d5e6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d923a07505d74ffd97f99ba063481963"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b4c1a6dbed964113b8c64e286732cad7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9c5770661f04f3885b3d8abd48c91d5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅█████████████</td></tr><tr><td>train_loss_epoch</td><td>█▃▁</td></tr><tr><td>train_loss_step</td><td>█▅▅▄▃▄▄▃▃▄▃▃▃▂▃▃▂▂▃▂▃▄▂▂▂▂▂▂▁▁▁▂▂▂▂▁▁▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▃▃▁▁▁▁▁▁▃▄▄▄▅▅▅▆▁▂▂▂▂▂▆▆▇▇▇██▂▂▂▂▂▂</td></tr><tr><td>val_loss_epoch</td><td>█▂▁</td></tr><tr><td>val_loss_step</td><td>▂▃▂▃▄█▇▆▂▄▃▄▃▅▃▂▄▄█▆▃▁▄▂▄▄▃▃▁▃▄█▁▃▁▄▄▃▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>train_loss_epoch</td><td>0.64387</td></tr><tr><td>train_loss_step</td><td>0.47519</td></tr><tr><td>trainer/global_step</td><td>1103</td></tr><tr><td>val_loss_epoch</td><td>1.06427</td></tr><tr><td>val_loss_step</td><td>1.03249</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">chocolate-sweep-8</strong> at: <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/08fyp9yp' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/08fyp9yp</a><br/> View project at: <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20240526_183011-08fyp9yp/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Agent Starting Run: tnycbwsl with config:\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tbatch_size: 8\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tcr_threshold: 1.2\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tlr: 6.346124608415621e-05\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tn_epochs: 3\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tunfreeze_decoder: True\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tunfreeze_encoder: False\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tunfreeze_linear: True\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \twarmup_steps: 800\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \tweight_decay: 0.004288391302419717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/home/tobias/Desktop/Uni/SS24/NLP/UKW_SpeachToText/wandb/run-20240526_183411-tnycbwsl</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/tnycbwsl' target=\"_blank\">happy-sweep-9</a></strong> to <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View sweep at <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/sweeps/ams4q60w</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/tnycbwsl' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/tnycbwsl</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/Desktop/Uni/SS24/NLP/UKW_SpeachToText/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/tobias/Desktop/Uni/SS24/NLP/UKW_SpeachToText/.venv/lib/python3.12/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Config item 'weight_decay' was locked by 'sweep' (ignored update).\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Config item 'warmup_steps' was locked by 'sweep' (ignored update).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c9b5a366f2bb423bbbb0a35320989e78"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cf33c925b6ed460ea2d21dab0dd1fdfc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "126d274d714d45728fccfb0aa7b9db5a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b1ed0576072b45f78a0de6e0cb1c1605"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9c1d0722e9494899b9a3699b1a3f185c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ce4b2168223743c3a8c313f4a4d8b858"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▅▅▅▅▅▅▅▅▅█████████████</td></tr><tr><td>train_loss_epoch</td><td>█▃▁</td></tr><tr><td>train_loss_step</td><td>█▆▅▅▄▄▅▃▃▄▃▃▃▂▃▃▂▂▃▂▃▅▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▂▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▂▂▂▃▃▁▁▁▁▁▁▃▄▄▄▅▅▅▆▁▂▂▂▂▂▆▆▇▇▇██▂▂▂▂▂▂</td></tr><tr><td>val_loss_epoch</td><td>█▅▁</td></tr><tr><td>val_loss_step</td><td>▃▃▂▃▄█▇▆▂▄▃▄▄▅▃▂▄▄█▆▄▂▄▃▄▅▄▃▂▃▄█▁▃▁▄▄▃▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>train_loss_epoch</td><td>0.7635</td></tr><tr><td>train_loss_step</td><td>0.50234</td></tr><tr><td>trainer/global_step</td><td>1103</td></tr><tr><td>val_loss_epoch</td><td>1.06434</td></tr><tr><td>val_loss_step</td><td>1.2008</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">happy-sweep-9</strong> at: <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/tnycbwsl' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny/runs/tnycbwsl</a><br/> View project at: <a href='https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/hp_tuning_whisper-tiny</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20240526_183411-tnycbwsl/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Sweep Agent: Waiting for job.\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gc\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "import optuna\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the search space for hyperparameters\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-4, 1e-3)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-4, 1e-3)\n",
    "    \n",
    "    parameters = {\n",
    "        \"n_epochs\": 3,\n",
    "        \"batch_size\": 8,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"warmup_steps\": 200,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"unfreeze_encoder\": False,\n",
    "        \"unfreeze_decoder\": True,\n",
    "        \"unfreeze_linear\": False\n",
    "    }\n",
    "    \n",
    "    # Initialize DataModule with the suggested batch size\n",
    "    data_module = SpeechDataModule(ds_train, ds_val, processor, parameters[\"batch_size\"])\n",
    "    \n",
    "    # Initialize the model with suggested hyperparameters\n",
    "    model_train = WhisperLightningModule(model_config[\"model_name\"], processor, parameters[\"learning_rate\"], parameters[\"weight_decay\"], parameters[\"warmup_steps\"])\n",
    "    model_train.model = WhisperForConditionalGeneration.from_pretrained(model_config[\"model_name\"])\n",
    "    model_train.model.generation_config.language = \"en\"\n",
    "    model_train.model.generation_config.task = \"transcribe\"\n",
    "    model_train.model.generation_config.is_multilingual = False\n",
    "    \n",
    "    # Freeze or unfreeze layers based on the original configuration\n",
    "    for param in model_train.model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    # Freeze layers in the decoder\n",
    "    for param in model_train.model.model.decoder.parameters():\n",
    "        param.requires_grad = parameters[\"unfreeze_decoder\"]\n",
    "        \n",
    "    # Freeze layers in the encoder\n",
    "    for param in model_train.model.model.encoder.parameters():\n",
    "        param.requires_grad = parameters[\"unfreeze_encoder\"]\n",
    "        \n",
    "    # Freeze layers in the linear layer\n",
    "    model_train.model.proj_out.weight.requires_grad = parameters[\"unfreeze_linear\"]\n",
    "    \n",
    "    \n",
    "    # Initialize the WandbLogger\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"ukw-radio-trans_\" + model_config[\"model_name\"].split(\"/\")[-1], \n",
    "        name=f\"lr_{parameters[\"learning_rate\"]:.6f}_wd_{parameters[\"weight_decay\"]:.6f}\",\n",
    "        log_model=False)\n",
    "    wandb_logger.log_hyperparams(parameters)\n",
    "    \n",
    "    # Initialize Early Stopping monitor the difference between the training and validation loss\n",
    "    early_stopping = pl.callbacks.EarlyStopping(\"val_loss\", patience=1, mode=\"min\", min_delta=0.1, verbose=False)\n",
    "    \n",
    "    # Initialize the Trainer with WandbLogger\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=parameters[\"n_epochs\"],\n",
    "        logger=wandb_logger,\n",
    "        accelerator=\"auto\",\n",
    "        log_every_n_steps=5,\n",
    "        num_sanity_val_steps=5,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.fit(model_train, data_module)\n",
    "    val_loss = trainer.callback_metrics[\"val_loss\"].item()\n",
    "    \n",
    "    # Finish the WandbLogger run\n",
    "    wandb.finish()\n",
    "    \n",
    "    # Free up memory\n",
    "    del model_train\n",
    "    del data_module\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Return the validation loss\n",
    "    return val_loss\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best trial:\")\n",
    "best_trial = study.best_trial\n",
    "print(f\"  Value: {best_trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ],
   "id": "d5ec35d0a892f380",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4b733ea6bf696128",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
