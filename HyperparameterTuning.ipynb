{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# UKW Marine Radio Chatter - Bridge 2 Bridge Communication\n",
    "This notebook uses pretrained models to transcribe the audio files from the UKW Marine Radio Chatter - Bridge 2 Bridge Communication dataset. <br>\n",
    "The dataset contains audio files and their corresponding transcriptions. Further we classify the speakers contained in the audio files."
   ],
   "id": "118ce1db4c0bea85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T16:38:33.670989Z",
     "start_time": "2024-05-25T16:38:32.691254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import IPython\n",
    "import torchaudio\n",
    "import torch\n",
    "import wandb\n",
    "from pydub import AudioSegment\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from src.utils import txt_to_dataframe\n",
    "import numpy as np"
   ],
   "id": "152a0b8bde3010d9",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configuration - Data Directories",
   "id": "fc6d817c34a305be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T16:38:33.675317Z",
     "start_time": "2024-05-25T16:38:33.672180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Config:\n",
    "    DATA_DIR = '../data/'\n",
    "    AUDIO_DIR = DATA_DIR + 'audio/'\n",
    "    TEXT_DIR = DATA_DIR + 'text/'\n",
    "    DATASET_DIR = 'dataset/'\n",
    "    \n",
    "    KAGGLE_DATA_TAG = 'linogova/marine-radio-chatter-bridge-2-bridge-communication/1'\n",
    "    KAGGLE_DATA_DIR = 'Marine_audio/'\n",
    "\n",
    "config = Config()"
   ],
   "id": "b08f2d6491009490",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T16:38:33.727413Z",
     "start_time": "2024-05-25T16:38:33.676570Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from scipy import signal\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "from joblib import Parallel, delayed\n",
    "from torch.utils.data import Dataset\n",
    "from src.utils import bcolors\n",
    "\n",
    "c = bcolors()\n",
    "\n",
    "def batch_data(data, max_duration=30):\n",
    "    batches = []\n",
    "    current_batch = []\n",
    "    curr_start = 0\n",
    "    \n",
    "    if len(data) == 0:\n",
    "        return []\n",
    "    \n",
    "    dur = data[-1][\"end_time\"] - data[0][\"start_time\"]\n",
    "    \n",
    "    if dur < 10:\n",
    "        return []\n",
    "\n",
    "    for entry in data:\n",
    "        if entry[\"end_time\"] - entry['start_time'] > max_duration:\n",
    "            \n",
    "            continue\n",
    "        if entry['end_time'] - curr_start + 0.2  > max_duration:\n",
    "            if len(current_batch) == 0:\n",
    "                curr_start = entry['start_time'] - 0.2\n",
    "            else:\n",
    "                batches.append(current_batch)\n",
    "                current_batch = []\n",
    "                curr_start = entry['start_time'] - 0.2\n",
    "\n",
    "        current_batch.append(entry)\n",
    "\n",
    "    if current_batch:\n",
    "        batches.append(current_batch)\n",
    "\n",
    "    return batches\n",
    "\n",
    "def inner_merge_batches(data):\n",
    "    return [{\n",
    "        'text': ' '.join([entry['transcript'] for entry in batch]),\n",
    "        'start': batch[0]['start_time'],\n",
    "        'end': batch[-1]['end_time']\n",
    "    } for i, batch in enumerate(data)]\n",
    "\n",
    "def lowpass_filter(audio_data, sr):\n",
    "    # Create a lowpass filter\n",
    "    b, a = signal.butter(4, 1300, 'low', fs=sr)\n",
    "    # Apply the lowpass filter\n",
    "    filtered_audio_data = signal.filtfilt(b, a, audio_data)\n",
    "    return filtered_audio_data\n",
    "\n",
    "def apply_rms_normalization(waveform):\n",
    "    rms_value = waveform.pow(2).mean().sqrt()  # Calculate RMS value of the waveform\n",
    "    target_rms = 0.1  # Example target RMS value\n",
    "    normalized_waveform = waveform * (target_rms / rms_value)  # Scale waveform to desired RMS value\n",
    "    return normalized_waveform\n",
    "\n",
    "class UKWFunkSprache(Dataset):\n",
    "    def __init__(self, \n",
    "                 file_ids, \n",
    "                 root_dir, \n",
    "                 proc=None,\n",
    "                 rms_norm=False,\n",
    "                 filter_data=False,\n",
    "                 n_jobs=-1):\n",
    "        self.feed_ids = file_ids\n",
    "        self.root_dir = root_dir\n",
    "        self.processor = proc\n",
    "        self.rms_norm = rms_norm\n",
    "        self.filter_data = filter_data\n",
    "\n",
    "        print(f\"\\n{c.OKGREEN}Preloading Samples...{c.ENDC}\")\n",
    "        print(f\"\\n{c.OKCYAN}Audio Files:         {len(self.feed_ids)}{c.ENDC}\")\n",
    "        print(f\"{c.OKCYAN}Jobs:                {n_jobs} {c.ENDC}\\n\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        result = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(self.process_file)(idx) for idx in range(len(self.feed_ids))\n",
    "        )\n",
    "        result = [item for sublist in result for item in sublist]\n",
    "        print(f\"\\n{c.OKGREEN}Preloading Complete!{c.ENDC}\")\n",
    "\n",
    "        self.audio_samples = [item['audio'] for item in result]\n",
    "        self.transcriptions = [item['transcript'] for item in result]\n",
    "        self.groups = [item['group'] for item in result]\n",
    "        \n",
    "        print(f\"{c.OKCYAN}Number of Samples:   {len(self.audio_samples)} {c.ENDC}\\n\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        t = end_time - start_time\n",
    "        print(f\"\\n{c.OKBLUE}Time taken:      {int((t - (t % 60)) / 60)} min {t % 60} sec {c.ENDC}\")\n",
    "\n",
    "    def process_file(self, idx):\n",
    "        feed_id = self.feed_ids[idx]\n",
    "        audio_fpath = os.path.join(self.root_dir, f\"audio/{feed_id}.wav\")\n",
    "        text_fpath = os.path.join(self.root_dir, f\"text/{feed_id}.csv\")\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(audio_fpath, channels_first=True)\n",
    "        waveform = waveform.float()\n",
    "        transcripts_df = pd.read_csv(text_fpath)\n",
    "        \n",
    "        if self.rms_norm:\n",
    "            waveform = apply_rms_normalization(waveform)\n",
    "        \n",
    "        if self.filter_data:\n",
    "            waveform = lowpass_filter(waveform, sample_rate)\n",
    "        \n",
    "        batches = batch_data(transcripts_df.to_dict(\"records\"))\n",
    "        if len(batches) == 0:\n",
    "            return []\n",
    "        \n",
    "        metadata = inner_merge_batches(batches)\n",
    "\n",
    "        sample_group = str(feed_id)\n",
    "        samples = []\n",
    "        for i in range(len(metadata)):\n",
    "            start_time = metadata[i]['start'] - 0.2\n",
    "            end_time = metadata[i]['end'] + 0.2\n",
    "            transcript = metadata[i]['text']\n",
    "\n",
    "            start_sample = int(start_time * sample_rate)\n",
    "            end_sample = int(end_time * sample_rate)\n",
    "\n",
    "            sample = waveform[:, start_sample:end_sample].squeeze()\n",
    "            \n",
    "            if self.processor:\n",
    "                sample = self.processor.feature_extractor(sample, sampling_rate=sample_rate, return_tensors=\"pt\").input_features.squeeze(0)\n",
    "                transcript = self.processor.tokenizer(transcript, return_tensors=\"pt\").input_ids.squeeze(0)\n",
    "\n",
    "            samples.append({\n",
    "                'group': sample_group,\n",
    "                'audio': sample,\n",
    "                'transcript': transcript\n",
    "            })\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        audio = self.audio_samples[idx]\n",
    "        transcript = self.transcriptions[idx]\n",
    "\n",
    "        return {\n",
    "            \"input_features\": audio,\n",
    "            \"labels\": transcript\n",
    "        }\n"
   ],
   "id": "d0d540d77cdf2be2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "2c2f8a248e0826c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T16:38:33.739306Z",
     "start_time": "2024-05-25T16:38:33.728903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchmetrics\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import pytorch_lightning as pl\n",
    "import evaluate\n",
    "\n",
    "\n",
    "class WhisperLightningModule(pl.LightningModule):\n",
    "    def __init__(self, model_name: str, processor, learning_rate: float, weight_decay: float, warmup_steps: int, num_jobs: int = 8):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.processor = processor\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.num_jobs = num_jobs\n",
    "        \n",
    "        self.model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "        self.wer = torchmetrics.text.wer.WordErrorRate()\n",
    "        self.val_preds = []\n",
    "        self.val_true = []\n",
    "\n",
    "    def forward(self, input_features, labels):\n",
    "        return self.model(input_features=input_features, labels=labels)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(batch[\"input_features\"], batch[\"labels\"])\n",
    "        loss = outputs.loss\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self(batch[\"input_features\"], batch[\"labels\"])\n",
    "        loss = outputs.loss\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        \n",
    "        self.val_preds.append(outputs.logits.argmax(-1))\n",
    "        self.val_true.append(batch[\"labels\"])\n",
    "        \n",
    "    def on_validation_epoch_end(self) -> None:\n",
    "        self.val_preds = Parallel(n_jobs=self.num_jobs)(\n",
    "            delayed(self.processor.decode)(pred, skip_special_tokens=True) for b in self.val_preds for pred in b\n",
    "        )\n",
    "        self.val_true = Parallel(n_jobs=self.num_jobs)(\n",
    "            delayed(self.processor.decode)(true, skip_special_tokens=True) for b in self.val_true for true in b\n",
    "        )\n",
    "        \n",
    "        wer = self.wer(self.val_preds, self.val_true)\n",
    "        self.log(\"val_wer\", wer, prog_bar=True)\n",
    "        \n",
    "        self.val_preds = []\n",
    "        self.val_true = []\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.warmup_steps,\n",
    "            num_training_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]"
   ],
   "id": "1270ef37127a0362",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T16:38:33.748977Z",
     "start_time": "2024-05-25T16:38:33.740821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "class SpeechDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_dataset, val_dataset, processor, batch_size: int, num_workers: int = 8):\n",
    "        super().__init__()\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.processor = processor\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, collate_fn=self.collate_fn, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, collate_fn=self.collate_fn, num_workers=self.num_workers)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, collate_fn=self.collate_fn, num_workers=self.num_workers)\n",
    "\n",
    "    def collate_fn(self, features):\n",
    "        input_features = [feature[\"input_features\"] for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            [{\"input_features\": input_feature} for input_feature in input_features],\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels = [feature[\"labels\"] for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            [{\"input_ids\": label} for label in labels],\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        if (labels[:, 0] == self.processor.tokenizer.pad_token_id).all():\n",
    "            labels = labels[:, 1:]\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n"
   ],
   "id": "334f9c383e50aef2",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Hyperparameter Finetuning",
   "id": "b5f054d8f6f189c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T16:38:34.036877Z",
     "start_time": "2024-05-25T16:38:33.750344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "model_config = {\n",
    "    \"model_name\": \"openai/whisper-tiny\",\n",
    "}\n",
    "\n",
    "# Initialize the processor\n",
    "processor = WhisperProcessor.from_pretrained(\n",
    "    model_config[\"model_name\"], \n",
    "    language='en', \n",
    "    task=\"transcribe\", \n",
    "    do_normalize=True, \n",
    "    sampling_rate=16000, \n",
    "    return_tensors=\"pt\", \n",
    "    device=\"cpu\",\n",
    "    local_files_only=True\n",
    ")"
   ],
   "id": "36f10e79ecde3ba",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tobias/Desktop/Uni/SS24/NLP/UKW_SpeachToText/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-25T16:40:37.666717Z",
     "start_time": "2024-05-25T16:38:37.657194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_config[\"filter_data\"] = False\n",
    "model_config[\"rms_norm\"] = False\n",
    "\n",
    "# Create the Datasets\n",
    "feed_ids = [f.replace(\".wav\", \"\") for f in os.listdir(config.DATASET_DIR + \"audio\")]\n",
    "\n",
    "ds_train = UKWFunkSprache(\n",
    "    feed_ids[:1700], \n",
    "    config.DATASET_DIR, \n",
    "    proc=processor, \n",
    "    filter_data=model_config[\"filter_data\"], \n",
    "    rms_norm=model_config[\"rms_norm\"]\n",
    ")\n",
    "ds_val = UKWFunkSprache(\n",
    "    feed_ids[1700:], \n",
    "    config.DATASET_DIR, \n",
    "    proc=processor, \n",
    "    filter_data=model_config[\"filter_data\"], \n",
    "    rms_norm=model_config[\"rms_norm\"]\n",
    ")\n",
    "\n",
    "model_config[\"num_train_samples\"] = len(ds_train)\n",
    "model_config[\"num_val_samples\"] = len(ds_val)"
   ],
   "id": "66e42bf085eb5ee5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001B[92mPreloading Samples...\u001B[0m\n",
      "\n",
      "\u001B[96mAudio Files:         1700\u001B[0m\n",
      "\u001B[96mJobs:                -1 \u001B[0m\n",
      "\n",
      "\n",
      "\u001B[92mPreloading Complete!\u001B[0m\n",
      "\u001B[96mNumber of Samples:   3422 \u001B[0m\n",
      "\n",
      "\n",
      "\u001B[94mTime taken:      1 min 45.1122784614563 sec \u001B[0m\n",
      "\n",
      "\u001B[92mPreloading Samples...\u001B[0m\n",
      "\n",
      "\u001B[96mAudio Files:         300\u001B[0m\n",
      "\u001B[96mJobs:                -1 \u001B[0m\n",
      "\n",
      "\n",
      "\u001B[92mPreloading Complete!\u001B[0m\n",
      "\u001B[96mNumber of Samples:   586 \u001B[0m\n",
      "\n",
      "\n",
      "\u001B[94mTime taken:      0 min 14.88439154624939 sec \u001B[0m\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-05-25T16:40:37.668218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "from transformers import WhisperForConditionalGeneration\n",
    "import optuna\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "def objective(trial):\n",
    "    # Define the search space for hyperparameters\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-4, 1e-2)\n",
    "    \n",
    "    parameters = {\n",
    "        \"n_epochs\": 2,\n",
    "        \"batch_size\": 8,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"warmup_steps\": 200,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"unfreeze_encoder\": False,\n",
    "        \"unfreeze_decoder\": True,\n",
    "        \"unfreeze_linear\": False\n",
    "    }\n",
    "    \n",
    "    # Initialize DataModule with the suggested batch size\n",
    "    data_module = SpeechDataModule(ds_train, ds_val, processor, parameters[\"batch_size\"])\n",
    "    \n",
    "    # Initialize the model with suggested hyperparameters\n",
    "    model_train = WhisperLightningModule(model_config[\"model_name\"], processor, parameters[\"learning_rate\"], parameters[\"weight_decay\"], parameters[\"warmup_steps\"])\n",
    "    model_train.model = WhisperForConditionalGeneration.from_pretrained(model_config[\"model_name\"])\n",
    "    model_train.model.generation_config.language = \"en\"\n",
    "    model_train.model.generation_config.task = \"transcribe\"\n",
    "    model_train.model.generation_config.is_multilingual = False\n",
    "    \n",
    "    # Freeze or unfreeze layers based on the original configuration\n",
    "    for param in model_train.model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    # Freeze layers in the decoder\n",
    "    for param in model_train.model.model.decoder.parameters():\n",
    "        param.requires_grad = parameters[\"unfreeze_decoder\"]\n",
    "        \n",
    "    # Freeze layers in the encoder\n",
    "    for param in model_train.model.model.encoder.parameters():\n",
    "        param.requires_grad = parameters[\"unfreeze_encoder\"]\n",
    "        \n",
    "    # Freeze layers in the linear layer\n",
    "    model_train.model.proj_out.weight.requires_grad = parameters[\"unfreeze_linear\"]\n",
    "    \n",
    "    \n",
    "    # Initialize the WandbLogger\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"ukw-radio-trans_\" + model_config[\"model_name\"].split(\"/\")[-1], \n",
    "        name=f\"lr_{parameters[\"learning_rate\"]:.6f}_wd_{parameters[\"weight_decay\"]:.6f}\",\n",
    "        log_model=False)\n",
    "    wandb_logger.log_hyperparams(parameters)\n",
    "    \n",
    "    # Initialize the Trainer with WandbLogger\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=parameters[\"n_epochs\"],\n",
    "        logger=wandb_logger,\n",
    "        accelerator=\"auto\",\n",
    "        log_every_n_steps=5,\n",
    "        num_sanity_val_steps=5,\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    trainer.fit(model_train, data_module)\n",
    "    val_loss = trainer.callback_metrics[\"val_loss\"].item()\n",
    "    \n",
    "    # Finish the WandbLogger run\n",
    "    wandb.finish()\n",
    "    \n",
    "    # Free up memory\n",
    "    del model_train\n",
    "    del data_module\n",
    "    del trainer\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Return the validation loss\n",
    "    return val_loss\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best trial:\")\n",
    "best_trial = study.best_trial\n",
    "print(f\"  Value: {best_trial.value}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")\n"
   ],
   "id": "d5ec35d0a892f380",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-25 18:40:37,746] A new study created in memory with name: no-name-b60596cc-f1a4-425b-ace5-b9c3686d4a5a\n",
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mtobias-ettling\u001B[0m (\u001B[33mtobias-ettling-wandb\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20240525_184040-401vco56</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tobias-ettling-wandb/ukw-radio-trans_whisper-tiny/runs/401vco56' target=\"_blank\">lr_0.000587_wd_0.004995</a></strong> to <a href='https://wandb.ai/tobias-ettling-wandb/ukw-radio-trans_whisper-tiny' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/tobias-ettling-wandb/ukw-radio-trans_whisper-tiny' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/ukw-radio-trans_whisper-tiny</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/tobias-ettling-wandb/ukw-radio-trans_whisper-tiny/runs/401vco56' target=\"_blank\">https://wandb.ai/tobias-ettling-wandb/ukw-radio-trans_whisper-tiny/runs/401vco56</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name  | Type                            | Params\n",
      "----------------------------------------------------------\n",
      "0 | model | WhisperForConditionalGeneration | 37.8 M\n",
      "1 | wer   | WordErrorRate                   | 0     \n",
      "----------------------------------------------------------\n",
      "9.6 M     Trainable params\n",
      "28.1 M    Non-trainable params\n",
      "37.8 M    Total params\n",
      "151.043   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a7424f8a2f3f4839852e81c1e2593386"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "84d6f30381de4013a12d9b7eb401fa0d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "721e5a85cc7248baad770bb970063530"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed56b80e69ad494e93259c18049b35c7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f1c78d5c566f4c539d949221183b5bc0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "4b733ea6bf696128",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
