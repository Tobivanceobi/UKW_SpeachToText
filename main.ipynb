{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# UKW Marine Radio Chatter - Bridge 2 Bridge Communication\n",
    "This notebook uses pretrained models to transcribe the audio files from the UKW Marine Radio Chatter - Bridge 2 Bridge Communication dataset. <br>\n",
    "The dataset contains audio files and their corresponding transcriptions. Further we classify the speakers contained in the audio files."
   ],
   "id": "118ce1db4c0bea85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T12:56:26.243772Z",
     "start_time": "2024-05-17T12:56:20.173286Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import IPython\n",
    "import torchaudio\n",
    "import torch\n",
    "from pydub import AudioSegment\n",
    "from pytorch_lightning.utilities.types import STEP_OUTPUT\n",
    "from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline\n",
    "from src.utils import txt_to_dataframe\n",
    "import numpy as np"
   ],
   "id": "152a0b8bde3010d9",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Configuration - Data Directories",
   "id": "fc6d817c34a305be"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T12:56:26.249989Z",
     "start_time": "2024-05-17T12:56:26.245908Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Config:\n",
    "    DATA_DIR = '../data/'\n",
    "    AUDIO_DIR = DATA_DIR + 'audio/'\n",
    "    TEXT_DIR = DATA_DIR + 'text/'\n",
    "    DATASET_DIR = 'dataset/'\n",
    "    \n",
    "    KAGGLE_DATA_TAG = 'linogova/marine-radio-chatter-bridge-2-bridge-communication/1'\n",
    "    KAGGLE_DATA_DIR = 'Marine_audio/'\n",
    "\n",
    "config = Config()"
   ],
   "id": "b08f2d6491009490",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T12:56:27.195131Z",
     "start_time": "2024-05-17T12:56:26.251351Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "audio_files = [d + \"/\" + f for d in os.listdir(config.AUDIO_DIR) for f in os.listdir(config.AUDIO_DIR + \"/\" + d) if f]\n",
    "text_files = [f.replace(\".mp3\", \".txt\") for f in audio_files]\n",
    "\n",
    "audio_files = [config.AUDIO_DIR + f for f in audio_files]\n",
    "text_files = [config.TEXT_DIR + f for f in text_files]\n",
    "\n",
    "audio_text_files = zip(audio_files, text_files)\n",
    "audio_text_files = [(a, t) for a, t in audio_text_files if os.path.isfile(a) and os.path.isfile(t)]\n",
    "\n",
    "df = pd.DataFrame(audio_text_files, columns=['audio_file', 'text_file'])\n",
    "df.head()"
   ],
   "id": "58fde009db715ea9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                    audio_file  \\\n",
       "0  ../data/audio/26694/26694-20231228-0414.mp3   \n",
       "1  ../data/audio/26694/26694-20231225-0133.mp3   \n",
       "2  ../data/audio/26694/26694-20231214-0023.mp3   \n",
       "3  ../data/audio/26694/26694-20231216-1710.mp3   \n",
       "4  ../data/audio/26694/26694-20231221-1759.mp3   \n",
       "\n",
       "                                    text_file  \n",
       "0  ../data/text/26694/26694-20231228-0414.txt  \n",
       "1  ../data/text/26694/26694-20231225-0133.txt  \n",
       "2  ../data/text/26694/26694-20231214-0023.txt  \n",
       "3  ../data/text/26694/26694-20231216-1710.txt  \n",
       "4  ../data/text/26694/26694-20231221-1759.txt  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>audio_file</th>\n",
       "      <th>text_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../data/audio/26694/26694-20231228-0414.mp3</td>\n",
       "      <td>../data/text/26694/26694-20231228-0414.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../data/audio/26694/26694-20231225-0133.mp3</td>\n",
       "      <td>../data/text/26694/26694-20231225-0133.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../data/audio/26694/26694-20231214-0023.mp3</td>\n",
       "      <td>../data/text/26694/26694-20231214-0023.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../data/audio/26694/26694-20231216-1710.mp3</td>\n",
       "      <td>../data/text/26694/26694-20231216-1710.txt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../data/audio/26694/26694-20231221-1759.mp3</td>\n",
       "      <td>../data/text/26694/26694-20231221-1759.txt</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T12:56:27.200412Z",
     "start_time": "2024-05-17T12:56:27.196538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_mp3_to_wav(in_fpath, out_fpath):\n",
    "    audio = AudioSegment.from_mp3(in_fpath)\n",
    "    audio = audio.set_frame_rate(16000)\n",
    "    audio.export(out_fpath, format=\"wav\")\n",
    "    return out_fpath\n",
    "\n",
    "\n",
    "# for index, row in df.head(1000).iterrows():\n",
    "#     wav_file_path = config.DATASET_AUDIO + row['audio_file'].split(\"/\")[-1].replace(\".mp3\", \".wav\")\n",
    "#     csv_file_path = config.DATASET_TEXT + row['text_file'].split(\"/\")[-1].replace(\".txt\", \".csv\")\n",
    "#     convert_mp3_to_wav(row['audio_file'], wav_file_path)\n",
    "# \n",
    "#     sample_df = txt_to_dataframe(row['text_file'])\n",
    "#     sample_df.to_csv(csv_file_path, index=False)"
   ],
   "id": "97914aecac241fb9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T12:56:27.225405Z",
     "start_time": "2024-05-17T12:56:27.203160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "from joblib import Parallel, delayed\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import WhisperProcessor\n",
    "from src.utils import bcolors\n",
    "\n",
    "c = bcolors()\n",
    "\n",
    "def batch_data(data, max_duration=30):\n",
    "    batches = []\n",
    "    current_batch = []\n",
    "    curr_start = 0\n",
    "\n",
    "    for entry in data:\n",
    "        if entry['end_time'] - curr_start + 0.2  > max_duration:\n",
    "            if len(current_batch) == 0:\n",
    "                curr_start = entry['start_time'] - 0.2\n",
    "            else:\n",
    "                batches.append(current_batch)\n",
    "                current_batch = []\n",
    "                curr_start = entry['start_time'] - 0.2\n",
    "\n",
    "        current_batch.append(entry)\n",
    "\n",
    "    if current_batch:\n",
    "        batches.append(current_batch)\n",
    "\n",
    "    return batches\n",
    "\n",
    "def inner_merge_batches(data):\n",
    "    return [{\n",
    "        'text': ' '.join([entry['transcript'] for entry in batch]),\n",
    "        'start': batch[0]['start_time'],\n",
    "        'end': batch[-1]['end_time']\n",
    "    } for i, batch in enumerate(data)]\n",
    "\n",
    "class UKWFunkSprache(Dataset):\n",
    "    def __init__(self, \n",
    "                 file_ids, \n",
    "                 root_dir, \n",
    "                 proc=None, \n",
    "                 n_jobs=-1):\n",
    "        self.feed_ids = file_ids\n",
    "        self.root_dir = root_dir\n",
    "        self.processor = proc\n",
    "\n",
    "        print(f\"\\n{c.OKGREEN}Preloading Samples...{c.ENDC}\")\n",
    "        print(f\"\\n{c.OKCYAN}Audio Files:         {len(self.feed_ids)}{c.ENDC}\")\n",
    "        print(f\"{c.OKCYAN}Jobs:                {n_jobs} {c.ENDC}\\n\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        result = Parallel(n_jobs=n_jobs)(\n",
    "            delayed(self.process_file)(idx) for idx in range(len(self.feed_ids))\n",
    "        )\n",
    "        result = [item for sublist in result for item in sublist]\n",
    "        print(f\"\\n{c.OKGREEN}Preloading Complete!{c.ENDC}\")\n",
    "\n",
    "        self.audio_samples = [item['audio'] for item in result]\n",
    "        self.transcriptions = [item['transcript'] for item in result]\n",
    "        self.groups = [item['group'] for item in result]\n",
    "        \n",
    "        print(f\"{c.OKCYAN}Number of Samples:   {len(self.audio_samples)} {c.ENDC}\\n\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        t = end_time - start_time\n",
    "        print(f\"\\n{c.OKBLUE}Time taken:      {int((t - (t % 60)) / 60)} min {t % 60} sec {c.ENDC}\")\n",
    "\n",
    "    def process_file(self, idx):\n",
    "        feed_id = self.feed_ids[idx]\n",
    "        audio_fpath = os.path.join(self.root_dir, f\"audio/{feed_id}.wav\")\n",
    "        text_fpath = os.path.join(self.root_dir, f\"text/{feed_id}.csv\")\n",
    "\n",
    "        waveform, sample_rate = torchaudio.load(audio_fpath)\n",
    "        transcripts_df = pd.read_csv(text_fpath)\n",
    "        \n",
    "        if len(transcripts_df) < 5:\n",
    "            return []\n",
    "        \n",
    "        batches = batch_data(transcripts_df.to_dict('records'))\n",
    "        metadata = inner_merge_batches(batches)\n",
    "\n",
    "        sample_group = str(feed_id)\n",
    "        samples = []\n",
    "        for i in range(len(metadata)):\n",
    "            start_time = metadata[i]['start'] - 0.2\n",
    "            end_time = metadata[i]['end'] + 0.2\n",
    "            transcript = metadata[i]['text']\n",
    "            \n",
    "            if end_time - start_time < 10:\n",
    "                continue\n",
    "\n",
    "            start_sample = int(start_time * sample_rate)\n",
    "            end_sample = int(end_time * sample_rate)\n",
    "\n",
    "            sample = waveform[:, start_sample:end_sample].squeeze().numpy()\n",
    "            \n",
    "            \n",
    "            if self.processor:\n",
    "                sample = self.processor.feature_extractor(sample, sampling_rate=sample_rate, return_tensors=\"pt\").input_features.squeeze(0)\n",
    "                transcript = self.processor.tokenizer(transcript, return_tensors=\"pt\").input_ids.squeeze(0)\n",
    "\n",
    "            samples.append({\n",
    "                'group': sample_group,\n",
    "                'audio': sample,\n",
    "                'transcript': transcript\n",
    "            })\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        audio = self.audio_samples[idx]\n",
    "        transcript = self.transcriptions[idx]\n",
    "\n",
    "        return {\n",
    "            \"input_features\": audio,\n",
    "            \"labels\": transcript\n",
    "        }\n"
   ],
   "id": "d0d540d77cdf2be2",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model",
   "id": "2c2f8a248e0826c3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T12:56:27.240420Z",
     "start_time": "2024-05-17T12:56:27.226777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torchmetrics\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class WhisperLightningModule(pl.LightningModule):\n",
    "    def __init__(self, model_name: str, processor, learning_rate: float, weight_decay: float, warmup_steps: int):\n",
    "        super().__init__()\n",
    "        self.model = WhisperForConditionalGeneration.from_pretrained(model_name)\n",
    "        self.processor = processor\n",
    "        self.learning_rate = learning_rate\n",
    "        self.weight_decay = weight_decay\n",
    "        self.warmup_steps = warmup_steps\n",
    "        \n",
    "        self.wer = torchmetrics.text.wer.WordErrorRate()\n",
    "\n",
    "    def forward(self, input_features, labels):\n",
    "        return self.model(input_features=input_features, labels=labels)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(batch[\"input_features\"], batch[\"labels\"])\n",
    "        loss = outputs.loss\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self(batch[\"input_features\"], batch[\"labels\"])\n",
    "        loss = outputs.loss\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        outputs = self(batch[\"input_features\"], batch[\"labels\"])\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Compute WER for samples in the batch\n",
    "        pred_tok_ids = torch.argmax(outputs.logits, dim=-1)\n",
    "        dec_preds = self.processor.batch_decode(pred_tok_ids)\n",
    "        dec_labels = self.processor.batch_decode(batch[\"labels\"])\n",
    "        wer_score = wer(dec_preds, dec_labels)\n",
    "        \n",
    "        self.log(\"test_loss\", loss, prog_bar=True, on_step=False, on_epoch=True)\n",
    "        self.log(\"test_wer\", wer_score, prog_bar=True, on_step=False, on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.warmup_steps,\n",
    "            num_training_steps=self.trainer.estimated_stepping_batches\n",
    "        )\n",
    "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]"
   ],
   "id": "1270ef37127a0362",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T12:56:27.252373Z",
     "start_time": "2024-05-17T12:56:27.241795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class SpeechDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_dataset, val_dataset, processor, batch_size: int, num_workers: int = 8):\n",
    "        super().__init__()\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.processor = processor\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, collate_fn=self.collate_fn, num_workers=self.num_workers)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, collate_fn=self.collate_fn, num_workers=self.num_workers)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, collate_fn=self.collate_fn, num_workers=self.num_workers)\n",
    "\n",
    "    def collate_fn(self, features):\n",
    "        input_features = [feature[\"input_features\"] for feature in features]\n",
    "        batch = self.processor.feature_extractor.pad(\n",
    "            [{\"input_features\": input_feature} for input_feature in input_features],\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        labels = [feature[\"labels\"] for feature in features]\n",
    "        labels_batch = self.processor.tokenizer.pad(\n",
    "            [{\"input_ids\": label} for label in labels],\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "        if (labels[:, 0] == self.processor.tokenizer.pad_token_id).all():\n",
    "            labels = labels[:, 1:]\n",
    "        batch[\"labels\"] = labels\n",
    "        return batch\n"
   ],
   "id": "334f9c383e50aef2",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T12:56:27.263385Z",
     "start_time": "2024-05-17T12:56:27.253758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def print_parameter_table(model):\n",
    "    # Collect parameter information\n",
    "    parameter_info = []\n",
    "    total_params = 0\n",
    "    # only append parameters that require gradients, if they are frozen, summarize them as well and put them at the end\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            num_params = param.numel()\n",
    "            total_params += num_params\n",
    "            parameter_info.append([name, param.requires_grad, num_params])\n",
    "\n",
    "    # Print table\n",
    "    headers = [\"Parameter Name\", \"Requires Grad\", \"Num Params\"]\n",
    "    print(tabulate(parameter_info, headers=headers, tablefmt=\"grid\"))\n",
    "\n",
    "    # Print total number of parameters\n",
    "    print(f\"Total Parameters: {total_params}\")"
   ],
   "id": "322c78bdc7d6eeae",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Finetuning",
   "id": "b5f054d8f6f189c5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-17T12:56:27.689627Z",
     "start_time": "2024-05-17T12:56:27.264904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import WhisperProcessor\n",
    "\n",
    "# Initialize the processor\n",
    "processor = WhisperProcessor.from_pretrained(model_name, language='en', task=\"transcribe\")\n",
    "\n",
    "# Create the Datasets\n",
    "feed_ids = [f.replace(\".wav\", \"\") for f in os.listdir(config.DATASET_DIR + \"audio\")]\n",
    "\n",
    "ds_train = UKWFunkSprache(feed_ids[:800], config.DATASET_DIR, proc=processor)\n",
    "ds_val = UKWFunkSprache(feed_ids[800:1000], config.DATASET_DIR, proc=processor)"
   ],
   "id": "66e42bf085eb5ee5",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtransformers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m WhisperProcessor\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Initialize the processor\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m processor \u001B[38;5;241m=\u001B[39m WhisperProcessor\u001B[38;5;241m.\u001B[39mfrom_pretrained(\u001B[43mmodel_name\u001B[49m, language\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124men\u001B[39m\u001B[38;5;124m'\u001B[39m, task\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtranscribe\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Create the Datasets\u001B[39;00m\n\u001B[1;32m      7\u001B[0m feed_ids \u001B[38;5;241m=\u001B[39m [f\u001B[38;5;241m.\u001B[39mreplace(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.wav\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m os\u001B[38;5;241m.\u001B[39mlistdir(config\u001B[38;5;241m.\u001B[39mDATASET_DIR \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maudio\u001B[39m\u001B[38;5;124m\"\u001B[39m)]\n",
      "\u001B[0;31mNameError\u001B[0m: name 'model_name' is not defined"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "\n",
    "# Initialize processor and model name\n",
    "model_name = \"openai/whisper-tiny\"\n",
    "\n",
    "\n",
    "# Define training arguments\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 0.01\n",
    "warmup_steps = 100\n",
    "batch_size = 8\n",
    "num_epochs = 5"
   ],
   "id": "5724d7039e25e2c4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import WhisperForConditionalGeneration\n",
    "\n",
    "# Initialize DataModule\n",
    "data_module = SpeechDataModule(ds_train, ds_val, processor, batch_size)\n",
    "\n",
    "# Initialize the model\n",
    "model_train = WhisperLightningModule(model_name, processor, learning_rate, weight_decay, warmup_steps)\n",
    "\n",
    "model_train.model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n",
    "model_train.model.generation_config.language = \"en\"\n",
    "model_train.model.generation_config.task = \"transcribe\"\n",
    "\n",
    "# Freeze layers in the encoder\n",
    "for param in model_train.model.model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Freeze layers in the decoder\n",
    "for param in model_train.model.model.decoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the final linear layer for fine-tuning\n",
    "model_train.model.proj_out.weight.requires_grad = True\n",
    "\n",
    "# Set requires_grad to True to fine-tune embeddings\n",
    "for name, param in model_train.model.named_parameters():\n",
    "    if \"embed\" in name:\n",
    "        param.requires_grad = True  \n",
    "\n",
    "print_parameter_table(model_train.model)\n"
   ],
   "id": "cd01d16994d1f57d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from datetime import datetime\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# logger = WandbLogger(\n",
    "#     project=\"eurosat_\" + model_name,\n",
    "#     name=model_name,\n",
    "#     log_model=False,\n",
    "# )\n",
    "\n",
    "# Define callbacks (like ModelCheckpoint)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    dirpath=\"outputs/whisper-tiny/\" + datetime.now().strftime(\"%H-%M\"),\n",
    "    filename=\"{epoch:02d}-{train_loss_epoch:.2f}-{val_loss:.2f}\",\n",
    "    save_top_k=2,\n",
    "    mode=\"min\"\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=num_epochs,\n",
    "    callbacks=[checkpoint_callback],\n",
    "    logger=False,\n",
    "    accelerator=\"auto\",\n",
    "    log_every_n_steps=5\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model_train, data_module)"
   ],
   "id": "ae3d067d085b8404",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "trainer.test(model_train, data_module, ckpt_path=checkpoint_callback.best_model_path)",
   "id": "b759444e934517b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "model_test = WhisperLightningModule.load_from_checkpoint(\n",
    "    checkpoint_callback.best_model_path, \n",
    "    model_name=model_name, \n",
    "    processor=processor, \n",
    "    learning_rate=learning_rate, \n",
    "    weight_decay=weight_decay, \n",
    "    warmup_steps=warmup_steps\n",
    ")\n",
    "\n",
    "model_test.model.cuda()\n",
    "i = 0\n",
    "for batch in data_module.val_dataloader():\n",
    "    pred = model_test(batch[\"input_features\"].to(\"cuda\"), batch[\"labels\"].to(\"cuda\"))\n",
    "    pred_token_ids = torch.argmax(pred.logits, dim=-1)\n",
    "    decoded_labels = processor.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
    "    decoded_preds = processor.batch_decode(pred_token_ids, skip_special_tokens=True)\n",
    "    \n",
    "    wer = torchmetrics.text.wer.WordErrorRate()\n",
    "    wer(decoded_preds, decoded_labels)\n",
    "    print(wer.compute())\n",
    "    \n",
    "    print(\"Predicted:\\n\", decoded_preds[0])\n",
    "    print(\"True:\\n\", decoded_labels[0])\n",
    "    print()\n",
    "    i += 1\n",
    "    if i == 10:\n",
    "        break\n"
   ],
   "id": "d5f9c4cf44d0ec6b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "31bfa05de0f9f973",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
